#!/usr/bin/env python3
"""
Live E2E smoke test for the dual-pipeline EnvelopeDispatcher.

Runs transcripts through the full dispatcher, exercising both the Action Item
pipeline and the Deal pipeline concurrently against a single shared Neo4j
database — the same path production traffic follows.

Supports two data sources:
  - Seed DB mode (default): reads meetings from seed_meetings.db via a manifest
    generated by eq-structured-graph-core's ingest_stress_test.py. Covers 5
    accounts and ~35 meetings for cross-pipeline convergence testing.
  - Legacy mode (--legacy): reads 4 Lightbox transcripts from transcripts.json.

See docs/LIVE_E2E_TEST_RESULTS.md for the validation record.

Usage:
    python scripts/run_live_e2e.py
    python scripts/run_live_e2e.py --manifest ../eq-structured-graph-core/stress_test_manifest.json
    python scripts/run_live_e2e.py --legacy
"""

import argparse
import asyncio
import json
import os
import sqlite3
import sys
import time
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path
from uuid import UUID, uuid4

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from dotenv import load_dotenv
load_dotenv(Path(__file__).parent.parent / '.env')

from action_item_graph.clients.neo4j_client import Neo4jClient
from action_item_graph.clients.openai_client import OpenAIClient
from action_item_graph.clients.postgres_client import PostgresClient
from action_item_graph.models.envelope import (
    ContentFormat,
    ContentPayload,
    EnvelopeV1,
    InteractionType,
    SourceType,
)
from action_item_graph.pipeline import ActionItemPipeline

from deal_graph.clients.neo4j_client import DealNeo4jClient
from deal_graph.pipeline.pipeline import DealPipeline, DealPipelineResult

from dispatcher.dispatcher import EnvelopeDispatcher, DispatcherResult


# =============================================================================
# CLI Arguments
# =============================================================================


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Live E2E smoke test — dual-pipeline dispatcher"
    )
    parser.add_argument(
        '--manifest',
        type=str,
        default='../eq-structured-graph-core/stress_test_manifest.json',
        help='Path to stress_test_manifest.json (default: ../eq-structured-graph-core/stress_test_manifest.json)',
    )
    parser.add_argument(
        '--db-path',
        type=str,
        default='../eq-seed-test-data/seed_meetings.db',
        help='Path to seed_meetings.db (default: ../eq-seed-test-data/seed_meetings.db)',
    )
    parser.add_argument(
        '--legacy',
        action='store_true',
        help='Use legacy transcripts.json data source (single Lightbox account)',
    )
    return parser.parse_args()


# =============================================================================
# Data Loading — Seed DB (default)
# =============================================================================


def load_meetings_from_manifest(manifest_path: str, db_path: str) -> list[dict]:
    """Load meetings from seed DB using the manifest as the meeting list.

    The manifest (generated by eq-structured-graph-core's ingest_stress_test.py)
    records exactly which meetings were processed. We re-read those same meetings
    from the seed DB so both pipelines operate on identical data.

    Args:
        manifest_path: Path to stress_test_manifest.json
        db_path: Path to seed_meetings.db

    Returns:
        List of meeting dicts with full_transcript, sorted by date ascending
    """
    with open(manifest_path) as f:
        manifest = json.load(f)

    seed_ids = [m['seed_id'] for m in manifest['meetings']]

    conn = sqlite3.connect(f"file:{db_path}?mode=ro", uri=True)
    conn.row_factory = sqlite3.Row
    placeholders = ','.join('?' * len(seed_ids))
    rows = conn.execute(
        f"SELECT id, source_meeting_id, date, full_transcript, customer_name, account_id "
        f"FROM meetings WHERE id IN ({placeholders}) ORDER BY date",
        seed_ids,
    ).fetchall()
    conn.close()

    meetings = [dict(r) for r in rows]
    print(f"  Loaded {len(meetings)} meetings from seed DB via manifest")
    print(f"  Manifest tenant_id: {manifest['tenant_id']}")
    return meetings


def build_seed_envelope(meeting: dict, tenant_id: UUID) -> EnvelopeV1:
    """Build an EnvelopeV1 from a seed DB meeting row.

    Uses source_meeting_id as the interaction_id (deterministic UUID) so this
    pipeline MERGEs onto the same Interaction nodes that eq-structured-graph-core
    already created — proving cross-pipeline convergence.
    """
    return EnvelopeV1(
        tenant_id=tenant_id,
        user_id='auth0|live_test_user',
        pg_user_id=UUID('061ae392-47d5-4f04-9ea8-afa241f23555'),
        interaction_type=InteractionType.TRANSCRIPT,
        content=ContentPayload(
            text=meeting['full_transcript'],
            format=ContentFormat.DIARIZED,
        ),
        timestamp=datetime.fromisoformat(f"{meeting['date']}T12:00:00+00:00"),
        source=SourceType.API,
        interaction_id=UUID(meeting['source_meeting_id']),
        account_id=meeting['account_id'],
        extras={
            'meeting_title': f"{meeting['customer_name']} Meeting - {meeting['date']}",
            'user_name': "Peter O'Neil",
        },
    )


# =============================================================================
# Data Loading — Legacy (transcripts.json)
# =============================================================================


def load_transcripts() -> dict:
    """Load transcripts from the examples file (legacy mode)."""
    path = Path(__file__).parent.parent / 'examples' / 'transcripts' / 'transcripts.json'
    with open(path) as f:
        return json.load(f)


def build_legacy_envelope(transcript: dict, tenant_id: UUID, account_id: str) -> EnvelopeV1:
    """Build EnvelopeV1 from a legacy transcripts.json entry."""
    return EnvelopeV1(
        tenant_id=tenant_id,
        user_id='auth0|live_test_user',
        pg_user_id=UUID('061ae392-47d5-4f04-9ea8-afa241f23555'),
        interaction_type=InteractionType.TRANSCRIPT,
        content=ContentPayload(
            text=transcript['text'],
            format=ContentFormat.DIARIZED,
        ),
        timestamp=datetime.fromisoformat(transcript['timestamp']),
        source=SourceType.API,
        interaction_id=uuid4(),
        account_id=account_id,
        extras={
            'meeting_title': transcript['meeting_title'],
            'user_name': "Peter O'Neil",
        },
    )


# =============================================================================
# Database Cleanup
# =============================================================================


async def clean_database(neo4j: Neo4jClient, tenant_id: str, *, preserve_shared: bool = True):
    """Clean pipeline test data for this tenant.

    Args:
        neo4j: Neo4j client
        tenant_id: Tenant to clean
        preserve_shared: If True (default for seed mode), do NOT delete
            Interaction/Account nodes — they're shared with eq-structured-graph-core.
            If False (legacy mode), delete everything.
    """
    print("\n--- Cleaning shared database ---")

    # Pipeline-owned labels (always cleaned)
    queries = [
        "MATCH (n:ActionItemVersion) WHERE n.tenant_id = $tenant_id DETACH DELETE n",
        "MATCH (n:ActionItemTopicVersion) WHERE n.tenant_id = $tenant_id DETACH DELETE n",
        "MATCH (n:DealVersion) WHERE n.tenant_id = $tenant_id DETACH DELETE n",
        "MATCH (n:ActionItem) WHERE n.tenant_id = $tenant_id DETACH DELETE n",
        "MATCH (n:ActionItemTopic) WHERE n.tenant_id = $tenant_id DETACH DELETE n",
        "MATCH (n:Owner) WHERE n.tenant_id = $tenant_id DETACH DELETE n",
        "MATCH (n:Deal) WHERE n.tenant_id = $tenant_id DETACH DELETE n",
    ]

    # Shared skeleton labels — only delete in legacy mode
    if not preserve_shared:
        queries.extend([
            "MATCH (n:Interaction) WHERE n.tenant_id = $tenant_id DETACH DELETE n",
            "MATCH (n:Account) WHERE n.tenant_id = $tenant_id DETACH DELETE n",
        ])

    for query in queries:
        await neo4j.execute_write(query, {'tenant_id': tenant_id})

    # Verify clean state — check only the labels we actually deleted
    our_labels = [
        'ActionItemVersion', 'ActionItemTopicVersion', 'DealVersion',
        'ActionItem', 'ActionItemTopic', 'Owner', 'Deal',
    ]
    if not preserve_shared:
        our_labels.extend(['Interaction', 'Account'])

    label_filter = ' OR '.join(f'n:{label}' for label in our_labels)
    result = await neo4j.execute_query(
        f"MATCH (n) WHERE n.tenant_id = $tenant_id AND ({label_filter}) RETURN count(n) as count",
        {'tenant_id': tenant_id},
    )
    remaining = result[0]['count'] if result else 0

    mode = "preserving shared Interaction/Account" if preserve_shared else "full cleanup"
    print(f"  Cleaned pipeline labels ({mode}, {remaining} remaining)")
    if remaining > 0:
        raise RuntimeError(f"Database cleanup failed — {remaining} pipeline nodes remain")


# =============================================================================
# Transcript Processing
# =============================================================================


async def verify_deal_db_after_transcript(
    neo4j: DealNeo4jClient,
    tenant_id: str,
    account_id: str,
    transcript_index: int,
    interaction_id: str,
    meeting_title: str,
):
    """Count Account and Interaction nodes in Deal DB after a transcript run, and verify the exact Interaction."""
    acct_result = await neo4j.execute_query(
        "MATCH (a:Account {tenant_id: $tid, account_id: $aid}) RETURN count(a) as count",
        {'tid': tenant_id, 'aid': account_id},
    )
    int_result = await neo4j.execute_query(
        "MATCH (i:Interaction {tenant_id: $tid}) RETURN count(i) as count",
        {'tid': tenant_id},
    )
    acct_count = acct_result[0]['count'] if acct_result else 0
    int_count = int_result[0]['count'] if int_result else 0
    print(f"\n  [Deal DB Check] After transcript {transcript_index} ({meeting_title}):")
    print(f"    Accounts: {acct_count}  |  Interactions: {int_count}")

    # Query the exact Interaction node by its interaction_id
    exact_result = await neo4j.execute_query(
        """MATCH (i:Interaction {tenant_id: $tid, interaction_id: $iid})
           RETURN i.deal_count as deal_count, i.processed_at as processed_at,
                  i.interaction_type as interaction_type""",
        {'tid': tenant_id, 'iid': interaction_id},
    )
    if exact_result:
        row = exact_result[0]
        dc = row.get('deal_count', 'N/A')
        processed = 'Yes' if row.get('processed_at') else 'No'
        itype = row.get('interaction_type', 'unknown')
        print(f"    Interaction {interaction_id[:12]}...: deal_count={dc} | type={itype} | processed={processed}")
    else:
        print(f"    WARNING: Interaction {interaction_id} NOT FOUND in Deal DB!")


async def run_transcript(
    dispatcher: EnvelopeDispatcher,
    envelope: EnvelopeV1,
    meeting_title: str,
    sequence: int,
    index: int,
) -> dict:
    """Dispatch a single transcript envelope and capture results.

    Accepts a pre-built EnvelopeV1 so callers can use either seed or legacy
    envelope builders.
    """
    print(f"\n{'=' * 70}")
    print(f"TRANSCRIPT {index}: {meeting_title}")
    print("=" * 70)
    print(f"Sequence: {sequence}")
    print(f"Timestamp: {envelope.timestamp}")
    print(f"Account: {envelope.account_id}")
    print(f"Content length: {len(envelope.content.text)} characters")
    print(f"Preview: {envelope.content.text[:150]}...")
    print(f"Interaction ID: {envelope.interaction_id}")

    result: DispatcherResult = await dispatcher.dispatch(envelope)

    # ---- Action Item pipeline results ----
    print(f"\n--- Action Item Pipeline ---")
    if result.action_item_success:
        ai = result.action_item_result
        print(f"  Success: True")
        print(f"  Items extracted: {ai.total_extracted}")
        print(f"  Created: {len(ai.created_ids)}")
        print(f"  Updated: {len(ai.updated_ids)}")
        print(f"  Linked: {len(ai.linked_ids)}")
        print(f"  Topics created: {ai.topics_created}")
        print(f"  Topics linked: {ai.topics_linked}")
        print(f"  Processing time: {ai.processing_time_ms}ms")
        if ai.errors:
            print(f"  Errors: {ai.errors}")
    else:
        exc = result.action_item_result
        print(f"  Success: False")
        print(f"  Error: {type(exc).__name__}: {exc}")

    # ---- Deal pipeline results ----
    print(f"\n--- Deal Pipeline ---")
    if result.deal_success:
        deal = result.deal_result
        print(f"  Success: True")
        print(f"  Deals extracted: {deal.total_extracted}")
        print(f"  Deals created: {len(deal.deals_created)}")
        print(f"  Deals merged: {len(deal.deals_merged)}")
        print(f"  Processing time: {deal.processing_time_ms}ms")
        if deal.errors:
            print(f"  Per-deal errors: {deal.errors}")
        if deal.warnings:
            print(f"  Warnings: {deal.warnings}")
    else:
        exc = result.deal_result
        print(f"  Success: False")
        print(f"  Error: {type(exc).__name__}: {exc}")

    # ---- Dispatcher summary ----
    print(f"\n--- Dispatcher ---")
    print(f"  Overall success: {result.overall_success}")
    print(f"  Both succeeded: {result.both_succeeded}")
    print(f"  Dispatch time: {result.dispatch_time_ms}ms")
    if result.errors:
        print(f"  Dispatcher errors: {result.errors}")

    # Build summary dict for the aggregate report
    summary = {
        'meeting_title': meeting_title,
        'sequence': sequence,
        'interaction_id': str(envelope.interaction_id),
        'account_id': str(envelope.account_id),
        'overall_success': result.overall_success,
        'both_succeeded': result.both_succeeded,
        'dispatch_time_ms': result.dispatch_time_ms,
        # AI pipeline
        'ai_success': result.action_item_success,
        'ai_items': 0,
        'ai_created': 0,
        'ai_updated': 0,
        'ai_topics_created': 0,
        'ai_topics_linked': 0,
        'ai_time_ms': 0,
        # Deal pipeline
        'deal_success': result.deal_success,
        'deals_extracted': 0,
        'deals_created': 0,
        'deals_merged': 0,
        'deal_time_ms': 0,
        # Errors
        'errors': result.errors,
    }

    if result.action_item_success:
        ai = result.action_item_result
        summary.update({
            'ai_items': ai.total_extracted,
            'ai_created': len(ai.created_ids),
            'ai_updated': len(ai.updated_ids),
            'ai_topics_created': ai.topics_created,
            'ai_topics_linked': ai.topics_linked,
            'ai_time_ms': ai.processing_time_ms or 0,
        })

    if result.deal_success:
        deal = result.deal_result
        summary.update({
            'deals_extracted': deal.total_extracted,
            'deals_created': len(deal.deals_created),
            'deals_merged': len(deal.deals_merged),
            'deal_time_ms': deal.processing_time_ms or 0,
        })

    return summary


# =============================================================================
# Final State Queries
# =============================================================================


async def query_ai_final_state(neo4j: Neo4jClient, tenant_id: str, account_ids: list[str]) -> dict:
    """Query the final AI graph state across all accounts."""
    print("\n--- AI DATABASE: FINAL STATE ---")

    all_action_items = []
    all_topics = []
    all_owners = []
    all_interactions = []

    for account_id in account_ids:
        # Action Items
        ai_query = """
        MATCH (ai:ActionItem)
        WHERE ai.tenant_id = $tenant_id AND ai.account_id = $account_id
        OPTIONAL MATCH (ai)-[:OWNED_BY]->(o:Owner)
        OPTIONAL MATCH (ai)-[:BELONGS_TO]->(t:ActionItemTopic)
        OPTIONAL MATCH (ai)-[:EXTRACTED_FROM]->(i:Interaction)
        WITH ai, o.canonical_name as resolved_owner, t.name as topic_name,
             collect(DISTINCT i.title) as source_interactions
        RETURN ai.action_item_id as action_item_id,
               ai.summary as summary,
               ai.owner as owner,
               ai.owner_type as owner_type,
               ai.is_user_owned as is_user_owned,
               ai.status as status,
               ai.account_id as account_id,
               resolved_owner,
               topic_name,
               source_interactions
        ORDER BY ai.created_at
        """
        action_items = await neo4j.execute_query(ai_query, {
            'tenant_id': tenant_id,
            'account_id': account_id,
        })
        all_action_items.extend(action_items)

        # Topics
        topic_query = """
        MATCH (t:ActionItemTopic)
        WHERE t.tenant_id = $tenant_id AND t.account_id = $account_id
        OPTIONAL MATCH (ai:ActionItem)-[:BELONGS_TO]->(t)
        WITH t.name as name, t.action_item_count as count, collect(ai.summary) as linked_items
        RETURN name, count, linked_items
        ORDER BY name
        """
        topics = await neo4j.execute_query(topic_query, {
            'tenant_id': tenant_id,
            'account_id': account_id,
        })
        all_topics.extend(topics)

        # Interactions
        int_query = """
        MATCH (i:Interaction)
        WHERE i.tenant_id = $tenant_id AND i.account_id = $account_id
        OPTIONAL MATCH (ai:ActionItem)-[:EXTRACTED_FROM]->(i)
        WITH i.title as title, i.timestamp as timestamp, count(ai) as action_item_count
        RETURN title, timestamp, action_item_count
        ORDER BY timestamp
        """
        interactions = await neo4j.execute_query(int_query, {
            'tenant_id': tenant_id,
            'account_id': account_id,
        })
        all_interactions.extend(interactions)

    # Owners are tenant-scoped, not account-scoped
    owner_query = """
    MATCH (o:Owner)
    WHERE o.tenant_id = $tenant_id
    OPTIONAL MATCH (ai:ActionItem)-[:OWNED_BY]->(o)
    WITH o.canonical_name as name, count(ai) as item_count
    RETURN name, item_count
    ORDER BY item_count DESC
    """
    all_owners = await neo4j.execute_query(owner_query, {
        'tenant_id': tenant_id,
    })

    # Print summary grouped by account
    acct_items = defaultdict(list)
    for ai in all_action_items:
        acct_items[ai.get('account_id', 'unknown')].append(ai)

    for account_id in account_ids:
        items = acct_items.get(account_id, [])
        if items:
            print(f"\n  Account {account_id[:12]}...: {len(items)} action items")
            for idx, ai in enumerate(items[:5], 1):
                print(f"    {idx}. {ai['summary'][:60]}")
            if len(items) > 5:
                print(f"    ... and {len(items) - 5} more")

    print(f"\n  Total Action Items: {len(all_action_items)}")
    print(f"  Total Topics: {len(all_topics)}")
    print(f"  Total Owners: {len(all_owners)}")
    print(f"  Total Interactions: {len(all_interactions)}")

    items_with_topics = sum(1 for ai in all_action_items if ai['topic_name'])
    total = len(all_action_items)
    if total > 0:
        pct = 100 * items_with_topics / total
        print(f"  Items with Topics: {items_with_topics}/{total} ({pct:.1f}%)")

    return {
        'action_items': all_action_items,
        'topics': all_topics,
        'owners': all_owners,
        'interactions': all_interactions,
    }


async def query_deal_final_state(
    neo4j: DealNeo4jClient, tenant_id: str, account_ids: list[str],
    all_results: list[dict],
) -> dict:
    """Query the final Deal graph state across all accounts."""
    print("\n--- DEAL DATABASE: FINAL STATE ---")

    all_deals = []
    for account_id in account_ids:
        deal_query = """
        MATCH (d:Deal)
        WHERE d.tenant_id = $tenant_id AND d.account_id = $account_id
        RETURN d.opportunity_id as opportunity_id,
               d.deal_ref as deal_ref,
               d.name as name,
               d.stage as stage,
               d.amount as amount,
               d.currency as currency,
               d.opportunity_summary as opportunity_summary,
               d.evolution_summary as evolution_summary,
               d.version as version,
               d.confidence as confidence,
               d.account_id as account_id,
               d.meddic_completeness as meddic_completeness,
               d.created_at as created_at
        ORDER BY d.created_at
        """
        deals = await neo4j.execute_query(deal_query, {
            'tenant_id': tenant_id,
            'account_id': account_id,
        })
        all_deals.extend(deals)

    # Print deals grouped by account
    deals_by_acct = defaultdict(list)
    for d in all_deals:
        deals_by_acct[d.get('account_id', 'unknown')].append(d)

    for account_id in account_ids:
        acct_deals = deals_by_acct.get(account_id, [])
        if acct_deals:
            print(f"\n  Account {account_id[:12]}...: {len(acct_deals)} deals")
            for i, deal in enumerate(acct_deals, 1):
                amount_str = f"${deal['amount']:,.0f}" if deal.get('amount') else 'N/A'
                completeness = deal.get('meddic_completeness', 0.0)
                completeness_pct = f"{completeness * 100:.0f}%" if completeness is not None else 'N/A'
                print(f"    {i}. {deal['name']}")
                print(f"       Stage: {deal['stage'] or 'N/A'} | Amount: {amount_str} | MEDDIC: {completeness_pct}")

    print(f"\n  Total Deals: {len(all_deals)}")

    # DealVersions
    version_query = """
    MATCH (dv:DealVersion)
    WHERE dv.tenant_id = $tenant_id
    RETURN dv.version_id as version_id,
           dv.deal_opportunity_id as deal_opportunity_id,
           dv.version as version,
           dv.name as name,
           dv.change_summary as change_summary,
           dv.changed_fields as changed_fields,
           dv.created_at as created_at
    ORDER BY dv.created_at
    """
    versions = await neo4j.execute_query(version_query, {'tenant_id': tenant_id})
    version_count = len(versions)
    print(f"  Total DealVersions: {version_count}")

    # Interactions — query each by exact interaction_id from results
    interactions = []
    print(f"\n  Interactions (deal-enriched, by sequence):")
    for r in sorted(all_results, key=lambda x: x['sequence']):
        iid = r['interaction_id']
        title = r['meeting_title']
        row_result = await neo4j.execute_query(
            """MATCH (i:Interaction {tenant_id: $tid, interaction_id: $iid})
               RETURN i.deal_count as deal_count,
                      i.interaction_type as interaction_type,
                      i.processed_at as processed_at,
                      i.timestamp as timestamp""",
            {'tid': tenant_id, 'iid': iid},
        )
        if row_result:
            row = row_result[0]
            dc = row.get('deal_count', 'N/A')
            itype = row.get('interaction_type', 'unknown')
            processed = 'Yes' if row.get('processed_at') else 'No'
            print(f"    {title[:40]}: deal_count={dc} | type={itype} | processed={processed}")
            interactions.append({
                'interaction_id': iid,
                'meeting_title': title,
                'sequence': r['sequence'],
                'deal_count': dc,
                'interaction_type': itype,
                'processed': processed,
            })
        else:
            print(f"    WARNING: {title}: Interaction {iid} NOT FOUND!")
            interactions.append({
                'interaction_id': iid,
                'meeting_title': title,
                'sequence': r['sequence'],
                'deal_count': None,
                'missing': True,
            })
    print(f"  Total Interactions: {len(interactions)}")

    return {
        'deals': all_deals,
        'deal_version_count': version_count,
        'interactions': interactions,
    }


# =============================================================================
# Cross-Pipeline MERGE Verification
# =============================================================================


async def verify_cross_pipeline_merge(
    neo4j: Neo4jClient,
    tenant_id: str,
    account_ids: list[str],
    all_results: list[dict],
) -> bool:
    """Verify that both pipelines MERGEd onto the same Account and Interaction nodes.

    Checks per-account that exactly ONE Account node exists (no duplicates),
    and that each Interaction was enriched by both pipelines.
    """
    print(f"\n{'=' * 70}")
    print("CROSS-PIPELINE MERGE VERIFICATION")
    print("=" * 70)

    passed = True

    # --- 1. Account convergence (one node per account_id) ---
    for account_id in account_ids:
        acct_result = await neo4j.execute_query(
            """
            MATCH (a:Account {tenant_id: $tenant_id, account_id: $account_id})
            RETURN a.account_id AS account_id,
                   a.tenant_id AS tenant_id,
                   a.name AS name
            """,
            {'tenant_id': tenant_id, 'account_id': account_id},
        )

        acct_count = len(acct_result)
        if acct_count == 1:
            acct = acct_result[0]
            print(f"\n  [PASS] Account {acct['name']}: exactly 1 node ({account_id[:12]}...)")
        elif acct_count == 0:
            print(f"\n  [FAIL] Account {account_id[:12]}...: 0 nodes found")
            passed = False
        else:
            print(f"\n  [FAIL] Account {account_id[:12]}...: {acct_count} nodes — MERGE created duplicates!")
            passed = False

    # --- 2. Interaction convergence ---
    print(f"\n  Interactions (both pipelines should enrich the same node):")

    interaction_pass_count = 0
    for r in sorted(all_results, key=lambda x: x['sequence']):
        iid = r['interaction_id']
        title = r['meeting_title']

        int_result = await neo4j.execute_query(
            """
            MATCH (i:Interaction {tenant_id: $tenant_id, interaction_id: $iid})
            RETURN i.interaction_id AS interaction_id,
                   i.account_id AS account_id,
                   i.action_item_count AS action_item_count,
                   i.deal_count AS deal_count,
                   i.interaction_type AS interaction_type,
                   i.content_text IS NOT NULL AS has_content,
                   i.timestamp IS NOT NULL AS has_timestamp,
                   i.processed_at IS NOT NULL AS has_processed_at
            """,
            {'tenant_id': tenant_id, 'iid': iid},
        )

        node_count = len(int_result)
        if node_count == 0:
            print(f"    [FAIL] {title[:40]}: Interaction {iid[:12]}... NOT FOUND")
            passed = False
            continue
        if node_count > 1:
            print(f"    [FAIL] {title[:40]}: {node_count} nodes — MERGE created duplicates!")
            passed = False
            continue

        i = int_result[0]
        ai_count = i.get('action_item_count')
        deal_count = i.get('deal_count')

        ai_ok = ai_count is not None
        deal_ok = deal_count is not None

        if ai_ok and deal_ok:
            status = "PASS"
            interaction_pass_count += 1
        elif ai_ok:
            status = "PARTIAL"
            detail = "AI pipeline wrote, Deal pipeline did not"
        elif deal_ok:
            status = "PARTIAL"
            detail = "Deal pipeline wrote, AI pipeline did not"
        else:
            status = "FAIL"
            detail = "Neither pipeline enriched this node"
            passed = False

        print(f"    [{status}] {title[:40]}: "
              f"action_item_count={ai_count}  deal_count={deal_count}")
        if status == "PARTIAL":
            print(f"           Note: {detail}")

    # --- 3. Summary ---
    total = len(all_results)
    print(f"\n  Summary: {interaction_pass_count}/{total} interactions enriched by both pipelines")

    if passed and interaction_pass_count == total:
        print(f"\n  [PASS] Cross-pipeline MERGE verification: ALL CHECKS PASSED")
    elif passed:
        print(f"\n  [WARN] Cross-pipeline MERGE verification: partial enrichment")
    else:
        print(f"\n  [FAIL] Cross-pipeline MERGE verification: FAILURES DETECTED")

    return passed


# =============================================================================
# Postgres Dual-Write Verification
# =============================================================================


async def query_postgres_state(postgres: PostgresClient, tenant_id: str) -> dict:
    """Query Postgres tables to verify dual-write data landed correctly."""
    print("\n--- POSTGRES DUAL-WRITE: FINAL STATE ---")

    results = {}
    queries = {
        'action_items': (
            "SELECT COUNT(*) as count FROM action_items "
            "WHERE graph_action_item_id IS NOT NULL AND tenant_id = :tid"
        ),
        'action_item_topics': (
            "SELECT COUNT(*) as count FROM action_item_topics "
            "WHERE tenant_id = :tid"
        ),
        'action_item_topic_memberships': (
            "SELECT COUNT(*) as count FROM action_item_topic_memberships "
            "WHERE tenant_id = :tid"
        ),
        'action_item_owners': (
            "SELECT COUNT(*) as count FROM action_item_owners "
            "WHERE tenant_id = :tid"
        ),
    }

    from sqlalchemy import text as sa_text

    for table, query in queries.items():
        try:
            async with postgres.engine.begin() as conn:
                row = await conn.execute(sa_text(query), {'tid': tenant_id})
                count = row.scalar() or 0
            results[table] = count
            print(f"  {table}: {count} rows")
        except Exception as e:
            results[table] = f"ERROR: {e}"
            print(f"  {table}: ERROR — {e}")

    # Spot-check a few action items
    try:
        async with postgres.engine.begin() as conn:
            rows = await conn.execute(
                sa_text(
                    "SELECT title, status, owner_name FROM action_items "
                    "WHERE graph_action_item_id IS NOT NULL AND tenant_id = :tid "
                    "LIMIT 5"
                ),
                {'tid': tenant_id},
            )
            samples = rows.fetchall()
        if samples:
            print(f"\n  Sample action items:")
            for row in samples:
                print(f"    - {row[0]} | status={row[1]} | owner={row[2]}")
    except Exception as e:
        print(f"  Sample query failed: {e}")

    return results


# =============================================================================
# Summary Report
# =============================================================================


def print_summary_report(
    results: list[dict],
    ai_state: dict,
    deal_state: dict,
    tenant_id: str,
    account_ids: list[str],
    total_time_ms: int,
):
    """Print the comprehensive summary report."""
    print("\n" + "=" * 70)
    print("COMPREHENSIVE SUMMARY REPORT")
    print("=" * 70)

    print(f"\nTenant ID: {tenant_id}")
    print(f"Accounts: {len(account_ids)}")
    print(f"Total wall-clock time: {total_time_ms}ms")

    # Per-transcript table
    print(f"\n--- PROCESSING SUMMARY ---")
    header = f"{'Transcript':<35} {'Account':<12} {'AI Items':<10} {'Deals':<8} {'Both OK':<9} {'Time':<10}"
    print(header)
    print("-" * len(header))

    t_items = t_deals = 0
    t_time = 0

    for r in results:
        title = r['meeting_title'][:33] + ".." if len(r['meeting_title']) > 35 else r['meeting_title']
        acct = r.get('account_id', '')[:10] + '..'
        both = "Yes" if r['both_succeeded'] else "No"
        print(
            f"{title:<35} {acct:<12} {r['ai_items']:<10} "
            f"{r['deals_created']:<8} {both:<9} {r['dispatch_time_ms']}ms"
        )
        t_items += r['ai_items']
        t_deals += r['deals_created']
        t_time += r['dispatch_time_ms'] or 0

    print("-" * len(header))
    print(f"{'TOTAL':<35} {'':<12} {t_items:<10} {t_deals:<8} {'':9} {t_time}ms")

    # Graph statistics
    print(f"\n--- AI GRAPH STATISTICS ---")
    print(f"  Action Items: {len(ai_state['action_items'])}")
    print(f"  Topics: {len(ai_state['topics'])}")
    print(f"  Owners: {len(ai_state['owners'])}")
    print(f"  Interactions: {len(ai_state['interactions'])}")

    print(f"\n--- DEAL GRAPH STATISTICS ---")
    print(f"  Deals: {len(deal_state['deals'])}")
    print(f"  DealVersions: {deal_state['deal_version_count']}")
    print(f"  Interactions: {len(deal_state['interactions'])}")

    # Error summary
    all_errors = [e for r in results for e in r['errors']]
    if all_errors:
        print(f"\n--- ERRORS ({len(all_errors)}) ---")
        for err in all_errors:
            print(f"  - {err}")
    else:
        print(f"\n  No errors across all transcripts.")

    # Overall verdict
    all_succeeded = all(r['both_succeeded'] for r in results)
    any_succeeded = all(r['overall_success'] for r in results)

    print("\n" + "=" * 70)
    if all_succeeded:
        print("TEST COMPLETE — ALL PIPELINES SUCCEEDED FOR ALL TRANSCRIPTS")
    elif any_succeeded:
        print("TEST COMPLETE — PARTIAL SUCCESS (at least one pipeline succeeded per transcript)")
    else:
        print("TEST COMPLETE — FAILURES DETECTED")
    print("=" * 70)


# =============================================================================
# Main
# =============================================================================


async def main():
    args = parse_args()

    # ---- Choose data source ----
    if args.legacy:
        # Legacy mode: transcripts.json with single Lightbox account
        data = load_transcripts()
        tenant_id = UUID(data['tenant_id'])
        tenant_id_str = str(tenant_id)
        account_ids = [data['account_id']]

        transcripts = sorted(data['transcripts'], key=lambda x: x['sequence'])
        meetings = None  # not used in legacy mode
        preserve_shared = False

        print("=" * 70)
        print("LIVE E2E SMOKE TEST — LEGACY MODE (transcripts.json)")
        print("=" * 70)
        print(f"Account: {data['account_name']}")
        print(f"Tenant ID: {tenant_id}")
        print(f"Account ID: {data['account_id']}")
        print(f"Transcripts to process: {len(transcripts)}")
    else:
        # Seed DB mode: read from manifest + SQLite
        manifest_path = Path(args.manifest)
        if not manifest_path.is_absolute():
            manifest_path = Path(__file__).parent.parent / manifest_path

        db_path = Path(args.db_path)
        if not db_path.is_absolute():
            db_path = Path(__file__).parent.parent / db_path

        if not manifest_path.exists():
            print(f"ERROR: Manifest not found: {manifest_path}")
            print("Run eq-structured-graph-core/scripts/ingest_stress_test.py first.")
            sys.exit(1)
        if not db_path.exists():
            print(f"ERROR: Seed DB not found: {db_path}")
            sys.exit(1)

        meetings = load_meetings_from_manifest(str(manifest_path), str(db_path))
        with open(manifest_path) as f:
            manifest = json.load(f)

        tenant_id = UUID(manifest['tenant_id'])
        tenant_id_str = str(tenant_id)
        account_ids = sorted({m['account_id'] for m in meetings})
        transcripts = None  # not used in seed mode
        preserve_shared = True

        print("=" * 70)
        print("LIVE E2E SMOKE TEST — SEED DB MODE (cross-pipeline convergence)")
        print("=" * 70)
        print(f"Tenant ID: {tenant_id}")
        print(f"Meetings: {len(meetings)}")
        print(f"Accounts: {len(account_ids)}")

        # Show per-account counts
        acct_counts = defaultdict(int)
        for m in meetings:
            acct_counts[m['customer_name']] += 1
        for name, count in sorted(acct_counts.items()):
            print(f"  {name}: {count} meetings")

    # ---- Initialize clients ----
    openai = OpenAIClient(api_key=os.getenv('OPENAI_API_KEY'))

    ai_neo4j = Neo4jClient(
        uri=os.getenv('NEO4J_URI'),
        username=os.getenv('NEO4J_USERNAME', 'neo4j'),
        password=os.getenv('NEO4J_PASSWORD'),
        database=os.getenv('NEO4J_DATABASE', 'neo4j'),
    )

    deal_neo4j = DealNeo4jClient(
        uri=os.getenv('DEAL_NEO4J_URI'),
        username=os.getenv('DEAL_NEO4J_USERNAME', 'neo4j'),
        password=os.getenv('DEAL_NEO4J_PASSWORD'),
        database=os.getenv('DEAL_NEO4J_DATABASE', 'neo4j'),
    )

    ai_pipeline = None
    dispatcher = None
    postgres = None

    try:
        # Connect and set up schemas
        await ai_neo4j.connect()
        await ai_neo4j.setup_schema()
        print("AI pipeline: connected and schema ready")

        await deal_neo4j.connect()
        await deal_neo4j.setup_schema()
        print("Deal pipeline: connected and enrichment schema ready")

        try:
            await deal_neo4j.verify_skeleton_schema()
            print("Shared database: skeleton schema verified")
        except RuntimeError as e:
            print(f"WARNING: Skeleton schema verification failed: {e}")
            print("Continuing anyway — MERGE-based operations may still work.")

        # Postgres dual-write (optional — skip if NEON_DATABASE_URL not set)
        neon_url = os.getenv('NEON_DATABASE_URL')
        if neon_url:
            postgres = PostgresClient(neon_url)
            await postgres.connect()
            if await postgres.verify_connectivity():
                print("Postgres (Neon): connected and ready for dual-write")
            else:
                print("WARNING: Postgres connectivity check failed — disabling dual-write")
                await postgres.close()
                postgres = None
        else:
            print("Postgres: NEON_DATABASE_URL not set — skipping dual-write")

        # Clean shared database
        print(f"\n{'=' * 70}")
        print("CLEANING SHARED DATABASE")
        print("=" * 70)
        await clean_database(ai_neo4j, tenant_id_str, preserve_shared=preserve_shared)

        # Build pipelines and dispatcher
        ai_pipeline = ActionItemPipeline(
            openai, ai_neo4j, enable_topics=True, postgres_client=postgres,
        )
        deal_pipeline = DealPipeline(deal_neo4j, openai, postgres_client=postgres)
        dispatcher = EnvelopeDispatcher(ai_pipeline, deal_pipeline)

        # ---- Process transcripts ----
        all_results = []
        t0 = time.monotonic()

        if args.legacy:
            # Legacy mode: single account, uuid4() interaction IDs
            account_id = account_ids[0]
            for i, transcript in enumerate(transcripts, 1):
                envelope = build_legacy_envelope(transcript, tenant_id, account_id)
                result = await run_transcript(
                    dispatcher, envelope,
                    meeting_title=transcript['meeting_title'],
                    sequence=transcript['sequence'],
                    index=i,
                )
                all_results.append(result)

                if not result['overall_success']:
                    print(f"\n  WARNING: Transcript {i} had failures!")

                await verify_deal_db_after_transcript(
                    deal_neo4j, tenant_id_str, account_id, i,
                    interaction_id=result['interaction_id'],
                    meeting_title=transcript['meeting_title'],
                )
        else:
            # Seed DB mode: multi-account, deterministic interaction IDs
            for i, meeting in enumerate(meetings, 1):
                envelope = build_seed_envelope(meeting, tenant_id)
                title = f"{meeting['customer_name']} Meeting - {meeting['date']}"
                result = await run_transcript(
                    dispatcher, envelope,
                    meeting_title=title,
                    sequence=i,
                    index=i,
                )
                all_results.append(result)

                if not result['overall_success']:
                    print(f"\n  WARNING: Meeting {i} had failures!")

                await verify_deal_db_after_transcript(
                    deal_neo4j, tenant_id_str, meeting['account_id'], i,
                    interaction_id=result['interaction_id'],
                    meeting_title=title,
                )

        total_time_ms = int((time.monotonic() - t0) * 1000)

        # Query final state from both databases
        print(f"\n{'=' * 70}")
        print("FINAL GRAPH STATE")
        print("=" * 70)
        ai_state = await query_ai_final_state(ai_neo4j, tenant_id_str, account_ids)
        deal_state = await query_deal_final_state(
            deal_neo4j, tenant_id_str, account_ids, all_results,
        )

        # Verify cross-pipeline MERGE convergence on shared labels
        merge_ok = await verify_cross_pipeline_merge(
            ai_neo4j, tenant_id_str, account_ids, all_results,
        )

        # Postgres dual-write verification
        pg_state = None
        if postgres is not None:
            print(f"\n{'=' * 70}")
            print("POSTGRES DUAL-WRITE VERIFICATION")
            print("=" * 70)
            pg_state = await query_postgres_state(postgres, tenant_id_str)

            # Compare Neo4j vs Postgres counts
            neo4j_ai_count = len(ai_state['action_items'])
            pg_ai_count = pg_state.get('action_items', 0)
            if isinstance(pg_ai_count, int):
                if pg_ai_count == neo4j_ai_count:
                    print(f"\n  [PASS] Action item count matches: Neo4j={neo4j_ai_count}, Postgres={pg_ai_count}")
                else:
                    print(f"\n  [WARN] Count mismatch: Neo4j={neo4j_ai_count}, Postgres={pg_ai_count}")

            # --- Deal dual-write verification ---
            from sqlalchemy import text
            async with postgres.engine.begin() as conn:
                result = await conn.execute(text(
                    "SELECT COUNT(*) FROM opportunities WHERE graph_opportunity_id IS NOT NULL"
                ))
                pg_deal_count = result.scalar()

                result = await conn.execute(text(
                    "SELECT COUNT(*) FROM deal_versions"
                ))
                pg_version_count = result.scalar()

            print(f"\n--- Postgres Deal Verification ---")
            print(f"Opportunities with graph_opportunity_id: {pg_deal_count}")
            print(f"Deal versions: {pg_version_count}")

            neo4j_deal_count = len(deal_state['deals'])
            if pg_deal_count == neo4j_deal_count:
                print(f"  [PASS] Deal count matches: Neo4j={neo4j_deal_count}, Postgres={pg_deal_count}")
            else:
                print(f"  [WARN] Deal count mismatch: Neo4j={neo4j_deal_count}, Postgres={pg_deal_count}")

            neo4j_version_count = deal_state['deal_version_count']
            if pg_version_count == neo4j_version_count:
                print(f"  [PASS] Deal version count matches: Neo4j={neo4j_version_count}, Postgres={pg_version_count}")
            else:
                print(f"  [WARN] Deal version count mismatch: Neo4j={neo4j_version_count}, Postgres={pg_version_count}")

        # Print comprehensive summary
        print_summary_report(
            all_results, ai_state, deal_state,
            tenant_id_str, account_ids, total_time_ms,
        )

        if pg_state is not None:
            print(f"\n--- POSTGRES DUAL-WRITE SUMMARY ---")
            for table, count in pg_state.items():
                print(f"  {table}: {count}")

    except Exception as e:
        print(f"\nFATAL ERROR: {e}")
        import traceback
        traceback.print_exc()
        raise

    finally:
        # Close clients — ai_pipeline.close() handles openai + ai_neo4j
        if ai_pipeline:
            await ai_pipeline.close()
        else:
            await ai_neo4j.close()
            await openai.close()
        await deal_neo4j.close()
        if postgres is not None:
            await postgres.close()


if __name__ == '__main__':
    asyncio.run(main())
